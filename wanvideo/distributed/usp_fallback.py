# Fallback implementation for USP when xFuser is not available
import torch
import torch.distributed as dist
from typing import Optional

# Global state for sequence parallel
_SEQUENCE_PARALLEL_SIZE = 1
_SEQUENCE_PARALLEL_RANK = 0
_SEQUENCE_PARALLEL_GROUP = None
_INITIALIZED = False

def initialize_sequence_parallel_fallback(sequence_parallel_size: int):
    """
    Fallback initialization for sequence parallel when xFuser is not available.
    This creates a lightweight implementation that works without complex distributed setup.
    """
    global _SEQUENCE_PARALLEL_SIZE, _SEQUENCE_PARALLEL_RANK, _SEQUENCE_PARALLEL_GROUP, _INITIALIZED

    print(f"ðŸ”§ Initializing USP fallback with sequence_parallel_size: {sequence_parallel_size}")

    # Always set the size for tracking purposes
    _SEQUENCE_PARALLEL_SIZE = max(1, sequence_parallel_size)
    _SEQUENCE_PARALLEL_RANK = 0  # Always rank 0 in fallback mode
    _SEQUENCE_PARALLEL_GROUP = None
    _INITIALIZED = True

    if sequence_parallel_size <= 1:
        print("âœ… USP fallback: Single GPU mode (no distributed required)")
        return

    # For multi-GPU, we track the size but don't require full distributed setup
    # The actual GPU distribution will be handled by model parallelism or other mechanisms
    print(f"âœ… USP fallback: Multi-GPU mode tracking {_SEQUENCE_PARALLEL_SIZE} GPUs")
    print("ðŸ’¡ Note: Full sequence parallelism requires xFuser. Using simplified version.")

    # Optional: Try to initialize distributed if available, but don't fail if not
    try:
        if not dist.is_initialized():
        #  and torch.cuda.is_available() and torch.cuda.device_count() > 1:
            import os
            os.environ.setdefault("MASTER_ADDR", "localhost")
            os.environ.setdefault("MASTER_PORT", "12356")  # Different port from FSDP
            dist.init_process_group(
                backend="nccl",
                rank=1,
                world_size=_SEQUENCE_PARALLEL_SIZE,
                init_method="env://",
                device_id=torch.device("cuda:1")
            )
            print("âœ… Distributed initialized for USP fallback")
    except Exception as e:
        print(f"âš ï¸  Distributed not available for USP fallback: {e}")
        print("   This is OK - using simplified sequence parallelism")

def get_sequence_parallel_world_size() -> int:
    """Return sequence parallel world size."""
    return _SEQUENCE_PARALLEL_SIZE

def get_sequence_parallel_rank() -> int:
    """Return sequence parallel rank."""
    return _SEQUENCE_PARALLEL_RANK

class MockSPGroup:
    """Mock sequence parallel group for fallback."""

    def __init__(self, world_size=1):
        self.world_size = world_size

    def all_gather(self, tensor: torch.Tensor, dim: int = 0) -> torch.Tensor:
        """Mock all_gather that handles sequence reconstruction."""
        # For fallback, we simulate the all_gather operation
        # In real sequence parallelism, tensors are chunked across GPUs and then gathered

        if self.world_size <= 1:
            # Single GPU: no gathering needed
            return tensor

        # Multi-GPU simulation: repeat the tensor to simulate gathering from multiple GPUs
        # This is a simplified version - real implementation would gather from actual GPUs
        expanded_shape = list(tensor.shape)
        expanded_shape[dim] = expanded_shape[dim] * self.world_size

        # Create a tensor that simulates gathering chunks from multiple GPUs
        result = tensor.repeat_interleave(self.world_size, dim=dim)
        print(f"ðŸ”„ USP fallback all_gather: {tensor.shape} -> {result.shape} (dim={dim})")
        return result

def get_sp_group():
    """Return sequence parallel group."""
    if _INITIALIZED:
        return MockSPGroup(_SEQUENCE_PARALLEL_SIZE)
    else:
        return MockSPGroup(1)

# Export the fallback functions
__all__ = [
    'initialize_sequence_parallel_fallback',
    'get_sequence_parallel_world_size',
    'get_sequence_parallel_rank',
    'get_sp_group'
]
